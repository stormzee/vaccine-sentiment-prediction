{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the data into a pandas dataframe..\n",
    "\n",
    "df_train = pd.read_csv('Train.csv')\n",
    "df_test = pd.read_csv('Test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>safe_text</th>\n",
       "      <th>label</th>\n",
       "      <th>agreement</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CL1KWCMY</td>\n",
       "      <td>Me &amp;amp; The Big Homie meanboy3000 #MEANBOY #M...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>E3303EME</td>\n",
       "      <td>I'm 100% thinking of devoting my career to pro...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>M4IVFSMS</td>\n",
       "      <td>#whatcausesautism VACCINES, DO NOT VACCINATE Y...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1DR6ROZ4</td>\n",
       "      <td>I mean if they immunize my kid with something ...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>J77ENIIE</td>\n",
       "      <td>Thanks to &lt;user&gt; Catch me performing at La Nui...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tweet_id                                          safe_text  label  \\\n",
       "0  CL1KWCMY  Me &amp; The Big Homie meanboy3000 #MEANBOY #M...    0.0   \n",
       "1  E3303EME  I'm 100% thinking of devoting my career to pro...    1.0   \n",
       "2  M4IVFSMS  #whatcausesautism VACCINES, DO NOT VACCINATE Y...   -1.0   \n",
       "3  1DR6ROZ4  I mean if they immunize my kid with something ...   -1.0   \n",
       "4  J77ENIIE  Thanks to <user> Catch me performing at La Nui...    0.0   \n",
       "\n",
       "   agreement  \n",
       "0        1.0  \n",
       "1        1.0  \n",
       "2        1.0  \n",
       "3        1.0  \n",
       "4        1.0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>safe_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5172</th>\n",
       "      <td>ZXVVNC5O</td>\n",
       "      <td>jenny mccarthy is on new years rockin eve. wha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5173</th>\n",
       "      <td>ZYIANVI8</td>\n",
       "      <td>Measles reported in Clark Co. for 1st time sin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5174</th>\n",
       "      <td>ZYITEHAH</td>\n",
       "      <td>&lt;user&gt; issues alert regarding Measles in TX. K...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5175</th>\n",
       "      <td>ZZ3BMBTG</td>\n",
       "      <td>I can't believe people don't vaccinate their k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5176</th>\n",
       "      <td>ZZIYCVNH</td>\n",
       "      <td>\"&lt;user&gt;  Alternatives to #Flu Vaccine &lt;url&gt; #n...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      tweet_id                                          safe_text\n",
       "5172  ZXVVNC5O  jenny mccarthy is on new years rockin eve. wha...\n",
       "5173  ZYIANVI8  Measles reported in Clark Co. for 1st time sin...\n",
       "5174  ZYITEHAH  <user> issues alert regarding Measles in TX. K...\n",
       "5175  ZZ3BMBTG  I can't believe people don't vaccinate their k...\n",
       "5176  ZZIYCVNH  \"<user>  Alternatives to #Flu Vaccine <url> #n..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 0.000000    4908\n",
       " 1.000000    4053\n",
       "-1.000000    1038\n",
       " 0.666667       1\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for value counts so as to see the number of classes in the target/output variable..\n",
    "\n",
    "df_train['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a multiclass classification problem since there are more than two classes. Here we have 0, 1, -1, and 0.667.\n",
    "But from the look of this dataset, we're supposed to predict between three(3) classes but we have four. This might be a data quality issue since the class 0.667 is not a valid class for this situation. It will be appropriate to remove that row from our dataset..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10001 entries, 0 to 10000\n",
      "Data columns (total 4 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   tweet_id   10001 non-null  object \n",
      " 1   safe_text  10001 non-null  object \n",
      " 2   label      10000 non-null  float64\n",
      " 3   agreement  9999 non-null   float64\n",
      "dtypes: float64(2), object(2)\n",
      "memory usage: 312.7+ KB\n"
     ]
    }
   ],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5177 entries, 0 to 5176\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   tweet_id   5177 non-null   object\n",
      " 1   safe_text  5176 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 81.0+ KB\n"
     ]
    }
   ],
   "source": [
    "df_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0,\n",
       " tweet_id     0\n",
       " safe_text    0\n",
       " label        1\n",
       " agreement    2\n",
       " dtype: int64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.duplicated().sum(), df_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0,\n",
       " tweet_id     0\n",
       " safe_text    1\n",
       " dtype: int64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.duplicated().sum(), df_test.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>safe_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2024</th>\n",
       "      <td>Dr. JAMES SHANNON</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               tweet_id safe_text\n",
       "2024  Dr. JAMES SHANNON       NaN"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df_test[df_test['safe_text'].isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, tweet_id DR JAMES SHANNON is not a valid tweet_id and so it needs to be dropped from the datasets.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seems to be few null values in both datasets.\n",
    "let's first drop the null values.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.dropna(axis=0, inplace=True)\n",
    "df_test.dropna(axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tweet_id     0\n",
       "safe_text    0\n",
       "label        0\n",
       "agreement    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tweet_id     0\n",
       "safe_text    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 0.0    4908\n",
       " 1.0    4053\n",
       "-1.0    1038\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, it appears that the value consisting of the null and invalid tweet_id was the same row which had an invalid class of 0.666667. it's been removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data cleaning of tweets\n",
    "1. Tokenization\n",
    "2. Removing stopwords, but in this case removing stopwords will be kinda disavantageous because stop words such as not, No will be needed to determine negative sentiments.\n",
    "3. Removing punctautions\n",
    "4. Lemmatization\n",
    "5. Stemming\n",
    "6. Part of speech tagging..\n",
    "6. Vectorization (TFID vectorizer) and so on..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "safe_text    CDC eyeing bird flu vaccine for humans, though...\n",
       "label                                                      0.0\n",
       "Name: 45, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[['safe_text','label']].loc[45]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>safe_text</th>\n",
       "      <th>label</th>\n",
       "      <th>agreement</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CL1KWCMY</td>\n",
       "      <td>Me &amp;amp; The Big Homie meanboy3000 #MEANBOY #M...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>E3303EME</td>\n",
       "      <td>I'm 100% thinking of devoting my career to pro...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>M4IVFSMS</td>\n",
       "      <td>#whatcausesautism VACCINES, DO NOT VACCINATE Y...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1DR6ROZ4</td>\n",
       "      <td>I mean if they immunize my kid with something ...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>J77ENIIE</td>\n",
       "      <td>Thanks to &lt;user&gt; Catch me performing at La Nui...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>IU0TIJDI</td>\n",
       "      <td>Living in a time where the sperm I used to was...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>WKKPCJY6</td>\n",
       "      <td>&lt;user&gt; &lt;user&gt;  In spite of all measles outbrea...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>ST3A265H</td>\n",
       "      <td>Interesting trends in child immunization in Ok...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>6Z27IJGD</td>\n",
       "      <td>CDC Says Measles Are At Highest Levels In Deca...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10000</th>\n",
       "      <td>P6190L3Q</td>\n",
       "      <td>Pneumonia vaccine: for women w risk of pulmona...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9999 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       tweet_id                                          safe_text  label  \\\n",
       "0      CL1KWCMY  Me &amp; The Big Homie meanboy3000 #MEANBOY #M...    0.0   \n",
       "1      E3303EME  I'm 100% thinking of devoting my career to pro...    1.0   \n",
       "2      M4IVFSMS  #whatcausesautism VACCINES, DO NOT VACCINATE Y...   -1.0   \n",
       "3      1DR6ROZ4  I mean if they immunize my kid with something ...   -1.0   \n",
       "4      J77ENIIE  Thanks to <user> Catch me performing at La Nui...    0.0   \n",
       "...         ...                                                ...    ...   \n",
       "9996   IU0TIJDI  Living in a time where the sperm I used to was...    1.0   \n",
       "9997   WKKPCJY6  <user> <user>  In spite of all measles outbrea...    1.0   \n",
       "9998   ST3A265H  Interesting trends in child immunization in Ok...    0.0   \n",
       "9999   6Z27IJGD  CDC Says Measles Are At Highest Levels In Deca...    0.0   \n",
       "10000  P6190L3Q  Pneumonia vaccine: for women w risk of pulmona...    1.0   \n",
       "\n",
       "       agreement  \n",
       "0       1.000000  \n",
       "1       1.000000  \n",
       "2       1.000000  \n",
       "3       1.000000  \n",
       "4       1.000000  \n",
       "...          ...  \n",
       "9996    1.000000  \n",
       "9997    0.666667  \n",
       "9998    1.000000  \n",
       "9999    1.000000  \n",
       "10000   0.666667  \n",
       "\n",
       "[9999 rows x 4 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We dont need the tweet_id and the agreement columns so it'll be dropped.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.drop(['tweet_id', 'agreement'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>safe_text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Me &amp;amp; The Big Homie meanboy3000 #MEANBOY #M...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I'm 100% thinking of devoting my career to pro...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#whatcausesautism VACCINES, DO NOT VACCINATE Y...</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I mean if they immunize my kid with something ...</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Thanks to &lt;user&gt; Catch me performing at La Nui...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>Living in a time where the sperm I used to was...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>&lt;user&gt; &lt;user&gt;  In spite of all measles outbrea...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>Interesting trends in child immunization in Ok...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>CDC Says Measles Are At Highest Levels In Deca...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10000</th>\n",
       "      <td>Pneumonia vaccine: for women w risk of pulmona...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9999 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               safe_text  label\n",
       "0      Me &amp; The Big Homie meanboy3000 #MEANBOY #M...    0.0\n",
       "1      I'm 100% thinking of devoting my career to pro...    1.0\n",
       "2      #whatcausesautism VACCINES, DO NOT VACCINATE Y...   -1.0\n",
       "3      I mean if they immunize my kid with something ...   -1.0\n",
       "4      Thanks to <user> Catch me performing at La Nui...    0.0\n",
       "...                                                  ...    ...\n",
       "9996   Living in a time where the sperm I used to was...    1.0\n",
       "9997   <user> <user>  In spite of all measles outbrea...    1.0\n",
       "9998   Interesting trends in child immunization in Ok...    0.0\n",
       "9999   CDC Says Measles Are At Highest Levels In Deca...    0.0\n",
       "10000  Pneumonia vaccine: for women w risk of pulmona...    1.0\n",
       "\n",
       "[9999 rows x 2 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's visualize the training dataset distribution..\n",
    "\n",
    "sns.countplot(data=df_train, x='label');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is balance between the labels zeros(0's) and ones(1's) but with very few instances of neutral sentiments, which makes it unbalanced for anti-vaccine and pro-vaccine.\n",
    "1 for pro vaccine(meaning they want the vaccine)\n",
    "-1 for anti-vaccine(they don't want to vaccinate)\n",
    "0 for neutral.\n",
    "Since the dataset is imbalanced, let's handle that before we start processing the tweet text. We'll handle imbalance using the SMOTE(synthetic Minority Oversampling Technique). Below are the steps to following when using the SMOTE.\n",
    "1. Choose a minority class as the input vector\n",
    "2. Find its k nearest neighbors (k_neighbors is specified as an argument in the SMOTE() function)\n",
    "3. Choose one of these neighbors and place a synthetic point anywhere on the line joining the point under consideration and its chosen neighbor\n",
    "4. Repeat the steps until data is balanced\n",
    "\n",
    "In a text dataset, SMOTE can't be applied from the start unless we do the vectorization of text and then convert them to numbers so that we can do the SMOTE to handle imbalanced dataset.. This will be done later in this project after getting the converting text to vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, let's plot the word cloud of this dataset..\n",
    "\n",
    "\n",
    "sentences_list = list(df_train['safe_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In building the wordcloud, we'll use the nltk stopwords function to capture all the stopwords in the wordcloud.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<user> a nearly 67 year old study when mental health studies and vaccines were relatively in their infancies that has been refuted?'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['safe_text'][5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "\n",
    "\n",
    "stopwords = STOPWORDS\n",
    "tokenizer = TweetTokenizer()\n",
    "\n",
    "\n",
    "def process_text(sentence):\n",
    "    \n",
    "    # remove punctuations\n",
    "    sentence = [i.lower() for i in sentence if i not in punctuation]\n",
    "    # rejoin the characters to form back the string(tweet)\n",
    "    sentence = ''.join(sentence)\n",
    "    # tokeninze the tweets\n",
    "    # remove non alphabets from tokens..\n",
    "    sentence = tokenizer.tokenize(sentence)\n",
    "    sentence = [i for i in sentence if i.isalpha()]\n",
    "    \n",
    "    # remove stopwords from the list..\n",
    "    # sentence = [i for i in sentence if i not in stopwords]\n",
    "    \n",
    "    # perform stemming/lemmatization\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = 'I owe; everything! to GOD.'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I owe everything to GOD'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "test = [char for char in test if char not in punctuation]\n",
    "test = ''.join(test)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_clean = df_train['safe_text'].apply(lambda x: process_text(x)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [me, amp, the, big, homie, meanboy, meanboy, m...\n",
       "1        [im, thinking, of, devoting, my, career, to, p...\n",
       "2        [whatcausesautism, vaccines, do, not, vaccinat...\n",
       "3        [i, mean, if, they, immunize, my, kid, with, s...\n",
       "4        [thanks, to, user, catch, me, performing, at, ...\n",
       "                               ...                        \n",
       "9996     [living, in, a, time, where, the, sperm, i, us...\n",
       "9997     [user, user, in, spite, of, all, measles, outb...\n",
       "9998     [interesting, trends, in, child, immunization,...\n",
       "9999     [cdc, says, measles, are, at, highest, levels,...\n",
       "10000    [pneumonia, vaccine, for, women, w, risk, of, ...\n",
       "Name: safe_text, Length: 9999, dtype: object"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'used', 'cedis', 'to', 'board', 'a', 'cab']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = 'i used 100 cedis to board a cab'\n",
    "process_text(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "# call the vectorizer and pass the text preprocessing function to it as the analyzer..\n",
    "vectorizer = TfidfVectorizer(analyzer=process_text)\n",
    "count_vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tweets_vectorizer = vectorizer.fit_transform(df_train_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ababyjustdiedinncbcofwhoopingcoughsogladmostpplindustinsampmyfamilyhavealreadygottenthevaccinegottatakeprecautions',\n",
       "       'ababywasbornonthejtrainthismorningcanwesafelyassumethatithasimmunitytoalldiseasesthataffecthumansandrats',\n",
       "       'abigthankyouonbehalfofmeaslestoallthedummieswhodontimmunizetheirkids',\n",
       "       ..., 'ºoºmaybeimmunizationmaybeitsautismºoº',\n",
       "       'вкололитутмнеmmrпрививкуmeaslesmumpsrubellavaccineвбудущеесмотрюсоптимизмом',\n",
       "       '病院実習行くのにmmrと水疱瘡の抗体を調べたらﾟдﾟ自分で保険会社に請求しろと言われた取り敢えず自己負担だね'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's convert to matrix of vectors\n",
    "\n",
    "train_X = tweets_vectorizer.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9999, 9569)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's create a new dataframe of these vectors.. as our training dataset..\n",
    "\n",
    "train_X = pd.DataFrame(train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>9559</th>\n",
       "      <th>9560</th>\n",
       "      <th>9561</th>\n",
       "      <th>9562</th>\n",
       "      <th>9563</th>\n",
       "      <th>9564</th>\n",
       "      <th>9565</th>\n",
       "      <th>9566</th>\n",
       "      <th>9567</th>\n",
       "      <th>9568</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9994</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9999 rows × 9569 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0     1     2     3     4     5     6     7     8     9     ...  9559  \\\n",
       "0      0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "1      0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "2      0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "3      0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "4      0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "...    ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  ...   ...   \n",
       "9994   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "9995   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "9996   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "9997   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "9998   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "\n",
       "      9560  9561  9562  9563  9564  9565  9566  9567  9568  \n",
       "0      0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "1      0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "2      0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "3      0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "4      0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "...    ...   ...   ...   ...   ...   ...   ...   ...   ...  \n",
       "9994   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "9995   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "9996   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "9997   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "9998   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "\n",
       "[9999 rows x 9569 columns]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = df_train['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        0.0\n",
       "1        1.0\n",
       "2       -1.0\n",
       "3       -1.0\n",
       "4        0.0\n",
       "        ... \n",
       "9996     1.0\n",
       "9997     1.0\n",
       "9998     0.0\n",
       "9999     0.0\n",
       "10000    1.0\n",
       "Name: label, Length: 9999, dtype: float64"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_X, train_y, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "mnb_clf = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = mnb_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lightGBM\n",
      "  Downloading lightgbm-3.3.2-py3-none-win_amd64.whl (1.0 MB)\n",
      "     ---------------------------------------- 1.0/1.0 MB 83.6 kB/s eta 0:00:00\n",
      "Requirement already satisfied: scipy in c:\\users\\sammy\\miniconda3\\envs\\ds\\lib\\site-packages (from lightGBM) (1.8.1)\n",
      "Requirement already satisfied: scikit-learn!=0.22.0 in c:\\users\\sammy\\miniconda3\\envs\\ds\\lib\\site-packages (from lightGBM) (1.1.1)\n",
      "Requirement already satisfied: wheel in c:\\users\\sammy\\miniconda3\\envs\\ds\\lib\\site-packages (from lightGBM) (0.37.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\sammy\\miniconda3\\envs\\ds\\lib\\site-packages (from lightGBM) (1.22.4)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\sammy\\miniconda3\\envs\\ds\\lib\\site-packages (from scikit-learn!=0.22.0->lightGBM) (3.1.0)\n",
      "Requirement already satisfied: joblib>=1.0.0 in c:\\users\\sammy\\miniconda3\\envs\\ds\\lib\\site-packages (from scikit-learn!=0.22.0->lightGBM) (1.1.0)\n",
      "Installing collected packages: lightGBM\n",
      "Successfully installed lightGBM-3.3.2\n"
     ]
    }
   ],
   "source": [
    "!pip install lightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip uninstall keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mitoinstallerNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading mitoinstaller-0.0.180.tar.gz (18 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting analytics-python\n",
      "  Using cached analytics_python-1.4.0-py2.py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\sammy\\miniconda3\\envs\\ds\\lib\\site-packages (from mitoinstaller) (0.4.4)\n",
      "Collecting termcolor\n",
      "  Downloading termcolor-2.0.1-py3-none-any.whl (5.4 kB)\n",
      "Collecting monotonic>=1.5\n",
      "  Using cached monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\sammy\\miniconda3\\envs\\ds\\lib\\site-packages (from analytics-python->mitoinstaller) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>2.1 in c:\\users\\sammy\\miniconda3\\envs\\ds\\lib\\site-packages (from analytics-python->mitoinstaller) (2.8.2)\n",
      "Collecting backoff==1.10.0\n",
      "  Using cached backoff-1.10.0-py2.py3-none-any.whl (31 kB)\n",
      "Collecting requests<3.0,>=2.7\n",
      "  Using cached requests-2.28.1-py3-none-any.whl (62 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sammy\\miniconda3\\envs\\ds\\lib\\site-packages (from requests<3.0,>=2.7->analytics-python->mitoinstaller) (2022.5.18.1)\n",
      "Collecting idna<4,>=2.5\n",
      "  Using cached idna-3.3-py3-none-any.whl (61 kB)\n",
      "Collecting urllib3<1.27,>=1.21.1\n",
      "  Downloading urllib3-1.26.12-py2.py3-none-any.whl (140 kB)\n",
      "     ------------------------------------ 140.4/140.4 kB 184.9 kB/s eta 0:00:00\n",
      "Collecting charset-normalizer<3,>=2\n",
      "  Downloading charset_normalizer-2.1.1-py3-none-any.whl (39 kB)\n",
      "Building wheels for collected packages: mitoinstaller\n",
      "  Building wheel for mitoinstaller (setup.py): started\n",
      "  Building wheel for mitoinstaller (setup.py): finished with status 'done'\n",
      "  Created wheel for mitoinstaller: filename=mitoinstaller-0.0.180-py3-none-any.whl size=25800 sha256=afdfe4262249a7651ee8f4552f926cc68a10f6d2ad5d96870fd0af9b8d5d5065\n",
      "  Stored in directory: c:\\users\\sammy\\appdata\\local\\pip\\cache\\wheels\\c1\\b6\\1b\\8b696b3e9af8623f6d8a64e40d0bc1f00b1aa24c84dc68be24\n",
      "Successfully built mitoinstaller\n",
      "Installing collected packages: monotonic, urllib3, termcolor, idna, charset-normalizer, backoff, requests, analytics-python, mitoinstaller\n",
      "Successfully installed analytics-python-1.4.0 backoff-1.10.0 charset-normalizer-2.1.1 idna-3.3 mitoinstaller-0.0.180 monotonic-1.6 requests-2.28.1 termcolor-2.0.1 urllib3-1.26.12\n"
     ]
    }
   ],
   "source": [
    "%pip install mitoinstaller\n",
    "# mitoinstaller install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3890676951.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [110]\u001b[1;36m\u001b[0m\n\u001b[1;33m    python -m mitoinstaller install\u001b[0m\n\u001b[1;37m              ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# from gettext import install\n",
    "\n",
    "\n",
    "python -m mitoinstaller install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert list of sentences into a single sentence..\n",
    "\n",
    "# all_sentences = ''.join(sentence.lower() for sentence in sentences_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wordcloud = WordCloud(width = 800, height = 800,\n",
    "#                 background_color ='white',\n",
    "#                 stopwords = stop_words,\n",
    "#                 min_font_size = 10).generate(all_sentences)\n",
    " \n",
    "# # plot the WordCloud image                      \n",
    "# plt.figure(figsize = (8, 8), facecolor = None)\n",
    "# plt.imshow(wordcloud)\n",
    "# plt.axis(\"off\")\n",
    "# plt.tight_layout(pad = 0)\n",
    " \n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above wordcloud plotted, some of the most mentioned  words are measle, vaccinate, vaccine, case, flu, polio, kid, parent, people, school, immunization, health, longer, ebola, immunity, children, vaccinating, autism. Some of the words are not associated with the vaccination but are related to the witter platform. For instance, user, url, link, amp, via and so on.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's do text preprocessing..\n",
    "# 1. Tokenization ==> spliting a sentence into its individual words\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # set x and y to the input data and output variable\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# x, y = df_train['safe_text'], df_train['label']\n",
    "\n",
    "# smote_technique = SMOTE()\n",
    "\n",
    "# x_smote, y_smote = smote_technique.fit_resample(x, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "28bd77a53f4f817a9e4a262195c721cc879ebcb57578d34de3cc1c8a75480f9f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
